package ja.com
import java.util.regex.Matcher

import ja.com.Common.cdxItem
import ja.conf.JobSparkConf
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.types.{LongType, StringType}
import org.jsoup.Jsoup

import scala.collection.mutable.ListBuffer

// import ja.conf._
/**
  * Created by Ja on 21/05/2017.
  */

object Job1 {
  case class t(x : Long, y: String)
  case class allLines( linenumber: Long  , isvalidheader:Boolean ,isheader:Boolean, url:String, urltime:String, mime:String , line:String)
  case class finalOutput(ln: String, p_urltime:String, p_mime:String, p_url:String, line:String)
  import java.util.regex.Pattern
  val dataLinePattern = Pattern.compile("(\\d{14})")

  def isCurrentLineIsValidHeader(line:String, lstMimes: List[String] ) : Boolean  = {

      if(lstMimes.exists(line.contains)) {
        val matcher: Matcher = dataLinePattern.matcher(line)
        matcher.find

    }
    else {
      false
    }
  }
  def isCurrentLineIsHeader(line:String) : Boolean  = {
      val valid1 = (line.split(" ").length > 2)
      val matcher: Matcher = dataLinePattern.matcher(line)
      matcher.find && valid1

  }

  def main(args: Array[String]): Unit = {

    val s = "com,google)/ 20090831083204 http://www.google.com/ text/html 302 MRIC3B6QBCMMI7MLO76A2P5ONACU4L7H - - 335 26267 EA-TNA-0709-biglotteryfund.org.uk-p-20090831083143-00000.arc.gz"
    val sp = s.split(" ")
    sp.foreach(println)

    // sc.textFile("myFile.gz")  sc.textFile("\*.gz")   // C:\Users\Ja\Google Drive\srcfile\srcfileloc

    val txtRDD = JobSparkConf.sc.textFile("C:\\Users\\Ja\\Google Drive\\srcfile\\srcfileloc\\*.*")
    val rddLines = txtRDD.mapPartitionsWithIndex { (idx, iter) => if (idx == 0) iter.drop(1) else iter }
    import JobSparkConf.sqlContext.implicits._
    val cdxItems = rddLines.map(x => cdxItem(x.split(" ")(0)
      ,x.split(" ")(1),x.split(" ")(2), x.split(" ")(3),
      x.split(" ")(4),x.split(" ")(5), x.split(" ")(6),
       x.split(" ")(7),x.split(" ")(8),x.split(" ")(9))
    ).toDF()
    //Â robots.txt, .js and, .json extensions, and any files which returned an HTTP status code of 400
    val filteredCDXitems = cdxItems
      .filter(!(cdxItems("a_origina_url").endsWith("robots.txt")
        || cdxItems("a_origina_url").endsWith(".js")
        || cdxItems("a_origina_url").endsWith(".json")
        ))

// s_response_code from 400  to 511
    import ja.com.Common._


    val filteredCDXitems1 = filteredCDXitems
      .filter(!(filteredCDXitems("s_response_code").between("400", "511")))
       .withColumn("New_URL", toGetURLnew(filteredCDXitems("a_origina_url")))

    // toGetURL(txtRDD).toString

    println(filteredCDXitems1)
  //  filteredCDXitems1.show(10000,truncate = false)
    filteredCDXitems1.registerTempTable("mainT")

    val inscopeDomains = JobSparkConf.sc.textFile("C:\\Users\\Ja\\Google Drive\\oldfiles\\Note\\srcfile\\SearchWhitelist\\dns_*")
    val domainsDf =   inscopeDomains.map( x => {
              if(x.indexOf("/") >= 0 )
                x.substring(0,x.indexOf("/")).replaceAll("\"", "")
              else
                x.replaceAll("\"", "")
    }).toDF("domain")


    println("count 1=" + domainsDf.count())
    domainsDf.registerTempTable("domainT")

  //  domainsDf.show(10)
    val joinedDF = JobSparkConf.sqlContext.sql("select mainT.* from mainT inner join domainT" +
      " ON mainT.New_URL LIKE concat('%',domainT.domain,'%') ")
    joinedDF.registerTempTable("joinedTable")
   // joinedDF.show(100)

    // 20090831084108 - image/jpeg - http://www.biglotteryfund.org.uk/index/newsroom-uk/openevent_08_2.jpg-

    /// Arc file starts here


    val txtArcRDD = JobSparkConf.sc.textFile("C:\\Users\\Ja\\Google Drive\\srcfile\\Note\\srcfile\\EA-TNA-0709-biglotteryfund.org.uk-p-20090831083143-00000.arc")
    val indexedArcRDD = txtArcRDD.zipWithIndex
    import JobSparkConf.sqlContext.implicits._
    val mainDF = indexedArcRDD.toDF("line", "linenumber")
    mainDF.registerTempTable("tmp1")
    val query  = "Select tmp1.linenumber + 1 linenumber, tmp1.line from tmp1 " +
      " join (select linenumber from tmp1 where trim(line) = '') as t2 " +
      " on tmp1.linenumber = t2.linenumber + 1 where trim(tmp1.line) <> '' "
      //" left join (select distinct m_mime_type_of_original_document mime from joinedTable) as t3 "
      //" on tmp1.line like concat('%',t3.mime,'%') where trim(tmp1.line) <> '' "

    val dfMimes = joinedDF.select(joinedDF("m_mime_type_of_original_document")).distinct().toDF("mime")
  //  dfMimes.registerTempTable("mimestable")
    //dfMimes.show(100)
    //dfMimes.count()   // TODO : 1
    //val broadcastVar = sc.broadcast(Array(1, 2, 3))
    var lstMimes = new ListBuffer[String]()
    val mlist1 = dfMimes.collect()
    println("fk")
    val mlist = mlist1.foreach(x => {
      println(x)
      lstMimes += x.getAs[String]("mime")
    })
    val broadcastVar = JobSparkConf.sc.broadcast(lstMimes.toList)

    val f1= JobSparkConf.sqlContext.sql(query).select("linenumber", "line").rdd

    val linenumbersBtwSpaces = f1.collect().map( x => x.getAs[Long]("linenumber"))
     println("line number for space lines")
   // linenumbersBtwSpaces.sorted.take(100).foreach(println)

    println("fk2")
    import java.util.regex.Pattern
    val pattern = Pattern.compile("(\\d{14})")

    val f4 = mainDF.map(x => {
      val ln = x.getAs[Long]("linenumber")
      val line = x.getAs[String]("line")
      var isvalidheader = isCurrentLineIsValidHeader(line, lstMimes.toList)
      var isheader = isCurrentLineIsHeader(line)
    var urltime = ""
      var url = ""
      var mimetype = ""
      if(isvalidheader){
          val arr = line.split(" ")
        url = arr(0)
        urltime = arr(2)
        mimetype = arr(3)
      }

      allLines( ln+1  , isvalidheader ,isheader,url,urltime,mimetype,  line)

    }).toDF().registerTempTable("fk4")
    println("key - values")
    val arcDF = JobSparkConf.sqlContext.sql("select * from fk4 order by linenumber")
  //  arcDF.show(10)
    //joinedTable
    val finalDF1 = JobSparkConf.sqlContext.sql("select distinct fk4.* from fk4 " +
      " join joinedTable on trim(joinedTable.b_date) = trim(fk4.urltime)  " +
      " and joinedTable.m_mime_type_of_original_document = trim(fk4.mime) " +
      " and fk4.url like concat('%',joinedTable.New_URL,'%')  " +
      "order by fk4.linenumber").toDF()
    finalDF1.persist()
    println("finalDF1 count =" + finalDF1.count())
    import org.apache.spark.sql.Row
    //|linenumber|isvalidheader|isheader|url  |urltime       |mime     |line |
    var initialDF = JobSparkConf.sqlContext.createDataFrame(JobSparkConf.sc.emptyRDD[Row], finalDF1.schema).withColumn("p_linenumberA", lit(null).cast(LongType))
      .withColumn("p_urltime", lit(null).cast(StringType)).withColumn("p_mime", lit(null).cast(StringType)).withColumn("p_url", lit(null).cast(StringType))
// it will take time. .this is silly logic but ok for POC :) ok
    val finalDF2 = finalDF1.collect().foreach(x => {
      val p_linenumberA = x.getAs[Long]("linenumber")
      val p_urltime = x.getAs[String]("urltime")
      val p_mime = x.getAs[String]("mime")
      val p_url = x.getAs[String]("url")
      // J alias name is missing its different

        val q2  = s"select min(linenumber) ln from fk4 where linenumber > $p_linenumberA and isheader = true"
        val minDF = JobSparkConf.sqlContext.sql(q2).toDF()
        val maxLineNo = minDF.map(t => t.getAs[Long]("ln")).collect().head
        // val q= s"select *,$p_linenumberA p_linenumberA from fk4 where linenumber >= $p_linenumberA and linenumber < $maxLineNo"
      val q= s"select * from fk4 where linenumber >= $p_linenumberA and linenumber < $maxLineNo"
        val finalDFInner = JobSparkConf.sqlContext.sql(q).toDF()
          .withColumn("p_linenumberA", lit(p_linenumberA).cast(LongType))
          .withColumn("p_urltime", lit(p_urltime).cast(StringType))
          .withColumn("p_mime", lit(p_mime).cast(StringType))
          .withColumn("p_url", lit(p_url).cast(StringType))

   //   finalDFInner.rdd.fold("")((s1, s2) => s1 + ", " + s2)

      initialDF = initialDF.unionAll(finalDFInner)

    })

    // udf = concat( p_linenumberA,  p_urltime,  p_mime, p_url )  ?
    // withColumn("new_col", udf())
    // or we can create new concat column in sql
    //initialDF.show(100, truncate = false)
    val fileOuptut = initialDF.map( x =>{
        val ln = x.getAs[Long]("p_linenumberA")
        val line = x.getAs[String]("line")
        val p_urltime = x.getAs[String]("p_urltime")
        val p_mime = x.getAs[String]("p_mime")
        val p_url = x.getAs[String]("p_url")
      if(p_mime.contains("image/jpeg") || p_mime.contains("image/gif"))
        {
          ((ln, p_urltime, p_mime, p_url), line)
        }
        else {
          ((ln, p_urltime, p_mime, p_url), line)
        }
    }).reduceByKey((iter, lineval) => {

      iter + " " + lineval
    })

var finalData = new ListBuffer[finalOutput]()
      fileOuptut.collect().foreach( x => {
        if(x._1._3.contains("text/html") ) {

          val doc1 = Jsoup.parse(x._2)
          val text = doc1.body.text
          finalData += finalOutput(x._1._1.toString  ,x._1._2 , x._1._3 , x._1._4 , text)
        }
        else if(x._1._3.contains("text/xml"))
          {

            val inputstring = x._2.substring( x._2.indexOf("<?xml version="))
            println("***********************************************************inut string " + inputstring)
            var finalstring = ""
            try {
              val raw = xml.XML.loadString(inputstring)
              raw.child.foreach(x => {
                if (!x.text.trim.isEmpty) finalstring += x.text + ","
              })

            }catch{
              case e:Exception => finalstring = "Error Parsing - Original Text:" + x._2
            }
            finalData += finalOutput(x._1._1.toString  ,x._1._2 , x._1._3 , x._1._4 , finalstring)
            //println(x._1._2 + " - " + x._1._3 + " - " + x._1._4 + "--------" + finalstring)
          }
        else if(x._1._3.contains("text/css") || x._1._3.contains("text/dns"))
          {
            //println(x._1._2 + " - " + x._1._3 + " - " + x._1._4 + "--------" + x._2)
            finalData += finalOutput(x._1._1.toString  ,x._1._2 , x._1._3 , x._1._4 , x._2)
          }
        else  // if(x._1._3.contains("image/jpeg") || x._1._3.contains("image/gif"))
        {
          //println(x._2)
          //val inputtext = x._2.substring(0, x._2.indexOf("  "))
          finalData += finalOutput(x._1._1.toString  ,x._1._2 , x._1._3 , x._1._4 , x._2)
        }
      //  finalData.coalesce(1).write.mode(SaveMode.Append).save("hdfs://139.162.54.153:8020/jatestfolder")
      }
    )

  //  finalData.coalesce(1).write.mode(SaveMode.Append).save("hdfs://139.162.54.153:8020/jatestfolder")
    //finalData.coalesce(1).write.mode(SaveMode.Append).save("hdfs://139.162.54.153:8020/jatestfolder")

    // 20090831084108 - image/jpeg - http://www.biglotteryfund.org.uk/index/newsroom-uk/openevent_08_2.jpg--------


   // finalDF2.count()
   // finalDF1.registerTempTable("cdxJoinArcTable")
   // finalDF1.printSchema()
   // finalDF1.show(100, truncate = false)

     //  finalData.saveAsTextFile("hdfs://127.0.01:8020/esopfolder")
    // finalData.saveAsTextFile("hdfs://139.162.54.153:8020/query6_dir1_jagan1")
    //finalData.saveAsTextFile("hdfs://139.162.54.153:8020/jatestfolder")

  }

}
